%!TEX root = ../msc_thesis.tex

\chapter{Variational Inference}
\label{ch:variational_inference}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% VI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Variational Inference}

As mentioned in Chapter \ref{ch:machine_learning}, when performing Bayesian inference, it is of interest to compute the posterior distribution $\prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}}$, which is usually intractable, so one must resort to numerical approximations. In this chapter, a brief overview of variational inference (VI) is given, which is one of several numerical approximations, such as Markov Chain Monte Carlo (MCMC) or Integrated Nested Laplace Approximations (INLA). The idea of VI is to use optimization to approximate the target distribution $\prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}}$ with some distribution $q(\boldsymbol{\theta})$ that is close to the posterior. The Kullback-Leibler (KL) divergence is used in VI as a measure of closeness of the proposed distribution, which is known as the variational approximation, to the true posterior \cite{blei2017variational}. The KL divergence is defined as
\begin{equation}
  \label{eq:kl_divergence}
  \kl{q}{p} = \mathbb{E}_q \left[ \log \left( \frac{q(\boldsymbol{\theta})}{\prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}}} \right) \right] = \int_{-\infty}^{\infty} q(\boldsymbol{\theta}) \left[ \log \left( \frac{q(\boldsymbol{\theta})}{\prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}}} \right) \right] d\boldsymbol{\theta}.
\end{equation}

Note that the KL divergence is not symmetrical, i.e., $\kl{q}{p} \neq \kl{p}{q}$. That is the reason it is called a divergence and not a distance. The former is called the \textbf{reverse KL divergence} and the latter is called the \textbf{forward KL divergence}, and each one of them prioritizes different aspects of the approximation. Reverse KL is said to be ``zero-forcing'' because it ensures that $q$ is zero in the areas in which $p$ is zero. On the contrary, forward KL is said to be ``zero-avoiding'' because $q$ avoids areas whose value is zero when $p$ also takes zero values.

This behavior can be appreciated in figure \ref{fig:KL_example}, which shows forward and reverse KL divergence of a mixture of Gaussians, denoted by $p$, and three different unimodal Gaussian distributions, each denoted by $q$. The blue distribution in the figure is the mixture of Gaussians, and the red distributions are the unimodal Gaussians.
% I cringe when I see this, but I didn't have time to make appropriate labels for the subfigures
In subfigure (a), the zero-forcing behavior can be appreciated because the red distribution $q$ has a mode where the blue distribution $p$ has zero values, and the reverse KL divergence is lower than the forward KL divergence.
In subfigures (b) and (c), the zero-avoiding behavior is evident because $q$ avoids the areas in which the mixture $p$ takes zero values, and because it has a lower forward KL divergence. These two subfigures also show the two local optima gotten from minimizing the reverse KL divergence, each being placed in the two modes of the $p$ distribution. In fact, both forward and reverse KL divergence on subfigure (b) are equal to the ones on subfigure (c).

\begin{figure}[H]
  \centering
  \subfloat[$\kl{p}{q} < \kl{q}{p}$]{\includegraphics[width=0.333\textwidth]{KL_example_1.pdf}}
  \hfill
  \subfloat[$\kl{p}{q} > \kl{q}{p}$]{\includegraphics[width=0.333\textwidth]{KL_example_2.pdf}}
  \hfill
  \subfloat[$\kl{p}{q} > \kl{q}{p}$]{\includegraphics[width=0.333\textwidth]{KL_example_3.pdf}}
  \caption{Comparison of forward and backward KL divergence. On the left, the forward KL divergence is smaller than the reverse KL divergence. On the center and on the right, the opposite happens.}
  \label{fig:KL_example}
\end{figure}

In VI, the goal is to minimize the reverse KL divergence $\kl{q}{p}$ instead of the forward KL divergence $\kl{p}{q}$ because the latter requires averaging with respect to $\prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}}$, which is what the modeler is trying to approximate in the first place. Methods to deal with forward KL divergence exist, such as expectation propagation, but they are not of concern in this work.

Although the main goal of VI methods is to minimize the reverse KL divergence defined in equation \eqref{eq:kl_divergence}, in practice what is usually done is to maximize a related quantity called the ELBO (Evidence Lower Bound), defined as
\begin{equation}
  \label{eq:elbo_def}
  \mathcal{L}(q) = \mathbb{E}_q\left[ \log p(\boldsymbol{y}, \boldsymbol{X}, \boldsymbol{\theta}) \right] - \mathbb{E}_q\left[ \log q(\boldsymbol{\theta}) \right].
\end{equation}

The relationship between the ELBO and KL is shown as follows. Starting from the KL divergence definition in \eqref{eq:kl_divergence}, using the quotient rule of the logarithm, and using the fact that taking expectations is a linear operator, the KL divergence can be written as
\begin{equation}
    \kl{q}{p} =
    \mathbb{E}_q \left[ \log \left( \frac{q(\boldsymbol{\theta})}{\prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}}} \right) \right] =
    \mathbb{E}_q \left[ \log  q(\boldsymbol{\theta}) \right] - \mathbb{E}_q \left[ \log {\prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}}}  \right].
\end{equation}

Using the definition of conditional probability yields
\begin{equation}
    \kl{q}{p} =
    \mathbb{E}_q \left[ \log  q(\boldsymbol{\theta}) \right] - \mathbb{E}_q \left[ \log \frac{p(\boldsymbol{\theta}, \boldsymbol{y} | \boldsymbol{X})}{p(\boldsymbol{y} | \boldsymbol{X})}  \right].
\end{equation}

Using the definition of conditional probability a second time, one obtains
\begin{equation}
    %\mathbb{E}_q \left[ \log  q(\boldsymbol{\theta}) \right] - \mathbb{E}_q \left[ \log p(\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X})  \right] =
    \kl{q}{p} =
    \mathbb{E}_q \left[ \log  q(\boldsymbol{\theta}) \right] - \mathbb{E}_q \left[ \log \frac{p(\boldsymbol{\theta}, \boldsymbol{y}, \boldsymbol{X})}{p(\boldsymbol{y} | \boldsymbol{X}) p(\boldsymbol{X})}  \right].
\end{equation}

Using the quotient rule of the logarithm one more time results in
\begin{equation}
  \kl{q}{p} =
  \mathbb{E}_q \left[ \log  q(\boldsymbol{\theta}) \right] - \mathbb{E}_q \left[ \log p(\boldsymbol{\theta}, \boldsymbol{y}, \boldsymbol{X}) - \log p(\boldsymbol{y} | \boldsymbol{X}) - \log p(\boldsymbol{X})  \right].
\end{equation}

But since $p(\boldsymbol{X})$ and $p(\boldsymbol{y} | \boldsymbol{X})$ do not depend on $q$, the expected value of each of these two densities is a constant under taking expectations under $q$, hence
\begin{equation}
 %\mathbb{E}_q \left[ \log  q(\boldsymbol{\theta}) \right] - \mathbb{E}_q \left[ \log p(\boldsymbol{\theta}, \boldsymbol{y} | \boldsymbol{X}) - \log p(X)  \right] =
 \kl{q}{p} =
 \mathbb{E}_q \left[ \log  q(\boldsymbol{\theta}) \right] - \mathbb{E}_q \left[ \log p(\boldsymbol{\theta}, \boldsymbol{y}, \boldsymbol{X}) \right] + \log p(\boldsymbol{y} | \boldsymbol{X}) + \log p(\boldsymbol{X}).
\end{equation}

Since the goal is to find the optimal $q(\cdot)$, then $\log p(\boldsymbol{X})$ and $\log p(\boldsymbol{y} | \boldsymbol{X})$ are just constants in the optimization process, hence they can be ignored for optimization purposes, and hence, one just needs to minimize the following equation
\begin{equation}
  \mathbb{E}_q \left[ \log  q(\boldsymbol{\theta}) \right] - \mathbb{E}_q \left[ \log p(\boldsymbol{\theta}, \boldsymbol{y}, \boldsymbol{X}) \right],
\end{equation}
which is the negative of the ELBO, defined in equation \eqref{eq:elbo_def}. Therefore, minimizing the KL divergence is equivalent to maximizing the ELBO.

Let $\boldsymbol{\lambda}$ be the parameter vector of the variational distribution $q(\boldsymbol{\theta})$. The objective is to approximate $p(\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X})$ by finding the values of $\boldsymbol{\lambda}$ that maximize equation \eqref{eq:elbo_def}. For this, several techniques could be used, such as coordinate ascent and gradient ascent. In order to perform gradient ascent, it is necessary to compute the gradient of the objective function with respect to the variational parameters, that is, $\nabla_{\boldsymbol{\lambda}} \mathcal{L}(q, \boldsymbol{\lambda})$. This gradient is computed by
\begin{equation}
  \label{eq:ELBO_gradient}
  \nabla_{\boldsymbol{\lambda}} \mathcal{L}(q, \boldsymbol{\lambda}) =
  \mathbb{E}_q \left[ \left( \nabla_{\boldsymbol{\lambda}} \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y}, \boldsymbol{X}, \boldsymbol{\theta}) - \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \right].
\end{equation}

In general, the gradient cannot be computed analytically because it may not be possible to write down an explicit formula for the expectation. It may be possible to take the expectation for some models, but for most cases, some approximation must be made. A good approach is to approximate the expected value with Monte Carlo simulation. Let $\left\{ \boldsymbol{z}_1, ..., \boldsymbol{z}_S \right\}$ be samples taken from $q(\boldsymbol{\theta} | \boldsymbol{\lambda})$, then one can approximate the expected value with an arithmetic mean as such,
\begin{equation}
  \begin{split}
  \nabla_{\boldsymbol{\lambda}} \mathcal{L}(q, \boldsymbol{\lambda}) &=
  \mathbb{E}_q \left[ \left( \nabla_{\boldsymbol{\lambda}} \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y}, \boldsymbol{X}, \boldsymbol{\theta}) - \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \right] \\
  & \approx \frac{1}{S} \sum_{k = 1}^S \left( \nabla_{\boldsymbol{\lambda}} \log q(\boldsymbol{z}_k | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y}, \boldsymbol{X}, \boldsymbol{z}_k) - \log q(\boldsymbol{z}_k | \boldsymbol{\lambda}) \right).
  \end{split}
\end{equation}

This approach gives noisy but unbiased estimates of the expected value. For more details about this see \cite{kucukelbir2017automatic} and \cite{ranganath2014black}.

A family that is often used to approximate $p$ is the \textbf{mean-field variational family}, where each parameter is independent from on another, such that
\begin{equation}
  q(\boldsymbol{\theta}) = \prod_{i} q(\theta_i | \lambda_i).
\end{equation}

% \section{Example}
The following example will help illustrate the logic behind the Monte Carlo approximation for the variational gradient under the mean field variational family. In this example, the goal is to approximate the posterior distribution of a two-class logistic regression. Assume a data matrix $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ and a response vector $\boldsymbol{y}$ with values 1 or 0. Each observation $y_i$ is modeled as a Bernoulli distribution such that
\begin{equation}
  y_i | \boldsymbol{x_i}, \boldsymbol{\theta} \sim \mathrm{Bern}(\sigma(\boldsymbol{\theta}^T \boldsymbol{x_i}))
\end{equation}
where $\sigma(\cdot)$ is the logistic sigmoid function. The prior distribution for $\boldsymbol{\theta}$ is a Gaussian $\boldsymbol{\theta} \sim \normaldist{\boldsymbol{0}}{\sigma_0^2 \boldsymbol{I}_p}$ where $\boldsymbol{I}_p$ is the $p \times p$ identity matrix and $\sigma_0^2 \in \mathbb{R}^+$.

In this case, the mean-field variational distribution is
\begin{equation}
  \label{eq:mean_field_normal_prior}
  q(\boldsymbol{\theta} | \boldsymbol{\lambda}) = \prod_{j = 1}^p \normalfunc{\theta_j}{\mu_j}{\sigma_j^2}
 \end{equation}
with $\boldsymbol{\lambda} = \left[ \mu_1, \hdots, \mu_p, \sigma_1^2, \hdots, \sigma_p^2 \right]^T$ and where $\normalfunc{\theta_j}{\mu_j}{\sigma_j^2}$ denotes the value of a Gaussian density function with mean $\mu_j$ and variance $\sigma_j^2$ that is evaluated in $\theta_j$.

According to equation \eqref{eq:ELBO_gradient}, to compute the gradient of the ELBO, one must be able to compute $\nabla_{\boldsymbol{\lambda}} \log q(\boldsymbol{\theta} | \boldsymbol{\lambda})$. Hence, it is necessary to have the partial derivative of $\log q(\boldsymbol{\theta} | \boldsymbol{\lambda})$ with respect to each $\mu_j$ and $\sigma_j^2$, with $j \in \left\{ 1, \hdots, p \right\}$.

The process to compute $\nabla_{\mu_j}$ goes as follows,
\begin{equation}
  \begin{split}
      \nabla_{\mu_j} \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) & =
      \nabla_{\mu_j} \log \left( \prod_{ k = 1}^p \normalfunc{\theta_k}{\mu_k}{\sigma_k^2} \right) \\
      &= \nabla_{\mu_j} \sum_{ k = 1}^p \log \left( \normalfunc{\theta_k}{\mu_k}{\sigma_k^2} \right) \\
      &= \nabla_{\mu_j} \log \left( \normalfunc{\theta_j}{\mu_j}{\sigma_j^2} \right) \\
      &= \nabla_{\mu_j} \log \left( \frac{1}{\sqrt{2 \pi \sigma_j^2}} \exp \left( -\frac{(\theta_j - \mu_j)^2}{2 \sigma_j^2} \right) \right) \\
      &= \frac{\theta_j - \mu_j}{\sigma_j^2}.
  \end{split}
\end{equation}

The process for $\sigma_j^2$ is similiar, although it is easier to work with the logarithm. Let $\alpha_j = \log \sigma_j^2$, so that instead of computing $\nabla_{\sigma_j^2}$, $\nabla_{\alpha_j}$ will be computed.

The process goes as follows,
\begin{equation}
  \begin{split}
      \nabla_{\alpha_j} \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) & =
      \nabla_{\alpha_j} \log \left( \prod_{ k = 1}^p \normalfunc{\theta_k}{\mu_k}{\sigma_k^2} \right) \\
      &= \nabla_{\alpha_j} \sum_{ k = 1}^p \log \left( \normalfunc{\theta_k}{\mu_k}{\sigma_k^2} \right) \\
      &= \nabla_{\alpha_j} \log \left( \normalfunc{\theta_j}{\mu_j}{\sigma_j^2} \right) \\
      &= \nabla_{\alpha_j} \log \left( \frac{1}{\sqrt{2 \pi \sigma_j^2}} \exp \left( -\frac{(\theta_j - \mu_j)^2}{2 \sigma_j^2} \right) \right) \\
      &= \nabla_{\alpha_j} \log \left( \frac{1}{\sqrt{2 \pi \mathrm{e}^{\alpha_j}}} \exp \left( -\frac{(\theta_j - \mu_j)^2}{2 \mathrm{e}^{\alpha_j}} \right) \right) \\
      &= - \nabla_{\alpha_j} \log \left( \sqrt{2 \pi \mathrm{e}^{\alpha_j}} \right) - \frac{(\theta_j - \mu_j)^2}{2} \nabla_{\alpha_j} \mathrm{e}^{-\alpha_j}\\
      &= - \frac{1}{2} + \frac{(\theta_j - \mu_j)^2}{2} \mathrm{e}^{-\alpha_j} \\
      &= - \frac{1}{2} + \frac{(\theta_j - \mu_j)^2}{2 \sigma_j^2}.
  \end{split}
\end{equation}

To compute the gradient in equation \eqref{eq:ELBO_gradient}, it is necessary to also compute the log-likelihood of the data $p(\boldsymbol{y}, \boldsymbol{X}, \boldsymbol{\theta})$. Using the chain rule of probability, this can be written as $\log p(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\theta}) + \log p(\boldsymbol{X} | \boldsymbol{\theta}) + \log p(\boldsymbol{\theta})$.

Note that in order to compute the gradient, $\log p(\boldsymbol{X} | \boldsymbol{\theta})$ is not needed, because in this case, the model is a discriminative model and not a generative one, hence $\boldsymbol{\theta}$ does not affect the distribution of the inputs $\boldsymbol{X}$. Mathematically, this can be seen in the following way
\begin{equation*}
  \begin{split}
      \nabla_{\boldsymbol{\lambda}} \mathcal{L} &=
      \mathbb{E}_q \left[ \left( \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y}, \boldsymbol{X}, \boldsymbol{\theta}) - \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \right] \\
      &= \mathbb{E}_q \left[ \left( \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\theta}) + \log p(\boldsymbol{X} | \boldsymbol{\theta}) + \log p(\boldsymbol{\theta}) - \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \right] \\
      &= \mathbb{E}_q \left[ \left( \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\theta}) + \log p(\boldsymbol{\theta}) - \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \right] +  \log p(\boldsymbol{X} | \boldsymbol{\theta}) \mathbb{E}_q \left[ \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right] \\
      &= \mathbb{E}_q \left[ \left( \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\theta}) + \log p(\boldsymbol{\theta}) - \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \right] +  c \mathbb{E}_q \left[ \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right].
  \end{split}
\end{equation*}
In the last line, $\log p(\boldsymbol{X} | \boldsymbol{\theta})$ has been reduced to just a constant $c$.

% Furthermore, $\mathbb{E}_q \left[ \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right] = 0$ because
% \begin{equation}
%   \mathbb{E}_q \left[ \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right] =
%       \int \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) d\boldsymbol{\theta}.
% \end{equation}

Furthermore, it can be proven that $\mathbb{E}_q \left[ \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right] = 0$ because of the definition of expectation and the dominated convergence theorem as was done in \cite{ranganath2014black},
\begin{equation}
  \int \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) d\boldsymbol{\theta} =
    \nabla_{\boldsymbol{\lambda}} \int q(\boldsymbol{\theta} | \boldsymbol{\lambda}) d\boldsymbol{\theta}.
\end{equation}

And finally,
\begin{equation}
    \nabla_{\boldsymbol{\lambda}} \int q(\boldsymbol{\theta} | \boldsymbol{\lambda}) d\boldsymbol{\theta} = 0.
\end{equation}
% \begin{equation}
%   \begin{split}
%       \nabla_{\boldsymbol{\lambda}} \int q(\boldsymbol{\theta} | \boldsymbol{\lambda}) d\boldsymbol{\theta} &=
%       \nabla_{\boldsymbol{\lambda}} 1 d\boldsymbol{\theta} \\
%       &= 0.
%   \end{split}
% \end{equation}

Therefore, the derivative of the ELBO can be written as
\begin{equation}
  \nabla_{\boldsymbol{\lambda}} \mathcal{L} = \mathbb{E}_q \left[ \left( \nabla_{\boldsymbol{\lambda}} q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\theta}) + \log p(\boldsymbol{\theta}) - \log q(\boldsymbol{\theta} | \boldsymbol{\lambda}) \right) \right].
\end{equation}

Since $\boldsymbol{y}$ is a Bernoulli random vector, then
\begin{equation}
  \begin{split}
      \log p(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\theta}) &=
      \log \left( \prod_{i = 1}^n \sigma(\boldsymbol{\theta}^T \boldsymbol{x_i})^{y_i} (1 - \sigma(\boldsymbol{\theta}^T \boldsymbol{x_i}))^{1-y_i} \right) \\
      &= \sum_{i = 1}^n \left[ y_i \log \left( \sigma(\boldsymbol{\theta}^T \boldsymbol{x_i} \right) + (1 - y_i) (1 - \sigma(\boldsymbol{\theta}^T \boldsymbol{x_i})) \right].
  \end{split}
\end{equation}

And since $\boldsymbol{\theta}$ was assumed to have Gaussian prior, then
\begin{equation}
  \log p(\boldsymbol{\theta}) = \log \prod_{j = 1}^p \phi(\theta_j) = \sum_{j = 1}^p \log \phi(\theta_j) = \frac{1}{\sqrt{2 \pi}} e^{\left( -\frac{\theta_j^2}{2} \right)}.
\end{equation}

With these derivations, a gradient ascent approach can be taken in order to optimize the ELBO. This is the same as gradient descent presented in Appendix \ref{ch:appendix}, except that instead of taking the negative of the gradient to move in the direction of maximum descent, the gradient is taken as it is so that the algorithm moves in the direction of maximum ascent. Like in gradient descent, the modeler starts with a random initial $\boldsymbol{\lambda}$ vector, and this vector is updated in each iteration using the gradient of the loss function. However, in this case, the gradient must be approximated with Monte Carlo simulation, so in each step $t$ the algorithm takes $S$ samples $\boldsymbol{z}_k$, with $\boldsymbol{z}_k \sim q(\boldsymbol{\theta} | \boldsymbol{\lambda})$ for $k \in \left\{1, \cdots, S \right\}$. This way, the approximate gradient is computed as
\begin{equation}
  \nabla_{\boldsymbol{\lambda}} \mathcal{L} \approx \Delta_{\boldsymbol{\lambda}} \mathcal{L} = \frac{1}{S} \sum_{k = 1}^S \left( \nabla_{\boldsymbol{\lambda}} \log q(\boldsymbol{z}_k | \boldsymbol{\lambda}) \right) \left( \log p(\boldsymbol{y}, \boldsymbol{X}, \boldsymbol{z}_k) - \log q(\boldsymbol{z}_k | \boldsymbol{\lambda}) \right),
\end{equation}
and then the $\boldsymbol{\lambda}$ parameter is updated as $\boldsymbol{\lambda}_{t+1} = \boldsymbol{\lambda}_{t} + \eta_t  \Delta_{\boldsymbol{\lambda}_t} \mathcal{L}$ until a certain stopping criteria is met, with each $\eta_t$ chosen beforehand. The choice of $\eta_t$ depends on the modeler, and can be chosen to be the same for all iterations, or it can change with each iteration like in the AdaGrad method \cite{duchi2011adaptive}.

An implementation of this algorithm was made in the R programming language with a simulated data set of $n = 500$ data points and $p = 1$ feature. The response vector was created with the logistic sigmoid function as $y_i = \sigma(\theta_0 + \theta_1 \boldsymbol{x_i})$, with $\theta_0 = -5$ and $\theta_1 = 5$.

The initial value for $\boldsymbol{\lambda} = \left[ \mu_1, \mu_2, \alpha_1, \alpha_2 \right]^T$ was chosen to be zero, and in each iteration, $S = 50$ Monte Carlo samples are taken to compute the approximate gradient. The step size $\boldsymbol{\eta}_t$ is chosen using the AdaGrad method \cite{duchi2011adaptive} in which
$\boldsymbol{\eta}_t = \left( \mathrm{diag}(\boldsymbol{G}_t) \right)^{\frac{1}{2}}$, where $\mathrm{diag}(\boldsymbol{G}_t)$
denotes the diagonal of the matrix $\boldsymbol{G}_t = \left( \Delta_{\boldsymbol{\lambda}_t} \right) \left( \Delta_{\boldsymbol{\lambda}_t} \right)^T$.
In this case $\boldsymbol{\eta}_t$ is a vector, hence the product $\boldsymbol{\eta}_t \Delta_{\boldsymbol{\lambda}_t}$ refers to an element-wise multiplication.
The stopping criterion was chosen to be
$\norm{\boldsymbol{\mu}_{t+1} - \boldsymbol{\mu}_{t}} < 0.005$,
where $\boldsymbol{\mu}_t = \left[ \mu_{1}^{t}, \mu_{2}^{t} \right]^T$.

Figure \ref{fig:BBVI_plots} shows the results of the implementation, and the values estimated by the \texttt{glm} package in R are also shown for comparison. The values estimated by the \texttt{glm} package are taken as Gold standard. The top-left image shows the value of the objective function, i.e., the ELBO, in each iteration, slowly increasing in zigzag because of the noise of the estimations. The top-right image shows the real values of $\boldsymbol{\theta}$ (-5 and 5) with gray dashed lines, the values of $\boldsymbol{\theta}$ estimated by the \texttt{glm} package with colored dotted lines, and the approximated values of the variational distribution $q(\boldsymbol{\theta} | \boldsymbol{\lambda})$.
The bottom-left image shows the values of $\mu_1$ and $\mu_2$ in each iteration, with horizontal colored dotted lines showing the estimated values by the \texttt{glm} package; it can be seen how they slowly approach their real values.
The bottom-right image shows the values of $\mu_1$ and $\mu_2$ in each iteration in the plane, where the noise of the gradient estimation is seen in the wiggliness of the line; the red dot shows once again the value estimated by the \texttt{glm} package.
% The results are decent for such a simple algorithm.
There are ways to have better and less noisy estimates, but they are beyond the scope of this work. For more details about better approximations of the gradient, see \cite{kucukelbir2017automatic} and \cite{ranganath2014black}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{BBVI_plots.pdf}
    \caption{Example of mean-field variational inference for Bayesian logistic regression.}
    \label{fig:BBVI_plots}
\end{figure}

The posterior predictive distribution of $y^*$ for a new vector of features $\boldsymbol{x}^*$ can be approximated using the variational approximation. In particular, the modeler could take $T$ samples from the variational posterior distribution as such: for $j \in \left\{ 1, \ldots, T \right\}$,
\begin{enumerate}
  \item Sample a vector $\boldsymbol{\theta}_j \sim q(\boldsymbol{\theta} | \boldsymbol{\lambda})$
  \item Use that value to sample $y_j^* \sim p(y^* | \boldsymbol{\theta}_j, \boldsymbol{x}^*)$
\end{enumerate}
Then $\left\{ y_j^* \right\}_{j = 1}^T$ is a set of $T$ independent samples from the posterior predictive distribution $\prob{y^* | \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{x}^*}$.

In this chapter, Variational Inference has been discussed as a way to approximate the posterior predictive distribution. This was posed as an optimization problem in which the Kullback-Leibler divergence is minimized, and it was shown to be equivalent to maximizing the ELBO. In addition, a numerical approximation to the gradient using Monte Carlo simulation was introduced, thus allowing the modeler to perform the optimization process. Finally, an example of VI on a logistic regression problem was presented. In the following chapter, a brief overview of Artificial Neural Network models will be presented to motivate the methodology presented in Chapter \ref{ch:active_learning} and illustrated with numerical experiments in Chapter \ref{ch:results}.
