%!TEX root = ../msc_thesis.tex

\chapter{Active Learning}
\label{ch:active_learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% AL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Active Learning}

An active learning problem is one in which the learner selects its own training data. For example, in a supervised learning problem where we are given a data set with features $X$ and response variable $y$. First we train a model $\hat{f}$ with the data that we have, afterwards the learner can choose new data $x^*$ from an unlabeled pool of data, ask to an \textit{oracle} what the corresponding output $y^*$ is, and then add the pair $(x^*, y^*)$ to the training data. The main goal of active learning is to select which $x^*$ to incorporate to the training data \cite{cohn1996active}.

The new example is chosen using an \textbf{acquisition function} $a(x, \hat{f})$ that is usually based on the model's uncertainty about the prediction. The new observation $x^*$ is chosen so that it maximizes the acquisition function of all the observations in the pool set. In practice, the oracle is usually a human that gives the corresponding label $y^*$. After the new observation is added to the training data, the model is retrained with the updated data set. This process is iteratively repeated, with the training set increasing in size with every iteration. In the end, it is expected that this procedure leads to a better predictive performance than randomly selecting new observations to add to the training data.

The acquisition functions that will be used and compared in this work are four, and three of them use the posterior predictive distribution, particularly, the posterior predictive probability of an observation $x^*$ having a label $y^*$ belonging to a class $c$, denoted as  $p(y^* = c | X, y, x^*)$. Naturally, because of the definition of these acquisition functions, they only work in classification frameworks.

\begin{enumerate}
  \item Predictive entropy:

  $ \mathbb{H} \left[ y^* | X, y, x^* \right] = - \sum_c p(y^* = c | X, y, x^*) \log p(y^* = c | X, y, x^*)$.

  \item Bayesian Active Learning by Disagreement (BALD):

  $ \mathbb{H} \left[ y^* | X, y, x^* \right] - \mathbb{E}_{p(\theta | X, y)} \left[ \mathbb{H} \left[ y^* | x^*, \theta \right] \right]$.

  \item Variation ratios: $1 - \max_y p(y^* | X, y, x^*)$.

  \item Random: Choosing an observation uniformly random from the pool of unlabeled data.

\end{enumerate}
