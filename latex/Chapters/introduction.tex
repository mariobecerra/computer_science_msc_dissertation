%!TEX root = ../msc_thesis.tex

\chapter{Introduction}
\label{ch:intro}

The main goal of this work is to compare approximate Bayesian methods with conventional methods used in Deep Learning within an Active Learning context. Conventional methods are based on the optimization of a loss function, which is rooted in maximum likelihood estimation. Optimization provides only a single point estimate with no measure of uncertainty, in particular for the parameters. On the other hand, the approximate Bayesian approach gives a joint posterior distribution of the parameters, thus giving a posterior predictive distribution of the response variable as well. It is well known that Bayesian methods avoid overfitting as they average over parameter values. However, due to their computational complexity, Bayesian methods are seldom used in Deep Learning, in contrast with conventional methods which are widely used.

Bayesian methods for Artificial Neural Networks date back to the late 1980s and early 1990s. These first approaches focused on Markov Chain Monte Carlo (MCMC) as
it generates, albeit asymptotically, samples from the posterior distribution
% the main method get samples of the posterior distribution
of the parameters, such as \citeauthor{neal1996bayesian} \cite{neal1996bayesian}, \citeauthor{denker1991transforming} \cite{denker1991transforming} and \citeauthor{mackay1992practical} \cite{mackay1992practical}.
% These approaches are not usually used because they are slow to train and do not scale well to the big Artificial Neural Networks and data sets used nowadays.
These approaches are not usually used in Machine Learning due to their highly intensive computational burden.
Nonetheless, there have been recent efforts in bridging Bayesian inference to Machine Learning through clever approximations to the full posterior distribution. An example of this, is the work by \citeauthor{hernandez2015probabilistic} \cite{hernandez2015probabilistic}.
% Lately there has been more work in the area, such as the Probabilistic Backpropagation algorithm developed by \citeauthor{hernandez2015probabilistic} \cite{hernandez2015probabilistic}.
This algorithm relies on one-dimensional Gaussian distributions that approximate the marginal posterior distribution of the parameters of a neural network at each step of training.
% weights on each iteration of backpropagation .
Another example is the work done by \citeauthor{graves2011practical} \cite{graves2011practical}, based on variational inference to approximate the posterior distribution of the parameters. However, these methods do not scale well to very big neural network architectures and data sets. They also have the disadvantage of only working with multi-layer perceptron architectures, making them impossible to use with more recent architectures such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs).

Very recently, \citeauthor{gal2015dropout1} \cite{gal2015dropout1} showed that a neural network with arbitrary depth, with dropout applied before each weight layer, is equivalent to the variational approximation to a deep Gaussian process. Dropout is the name of a widely used stochastic regularization technique, which will be discussed in Chapter \ref{ch:machine_learning}. The equivalence between a neural network that uses dropout and a deep Gaussian process comes from the fact that the loss function minimizes the Kullback Leibler divergence between an approximate distribution and the posterior of a deep Gaussian process. This means that approximate uncertainty estimates can be obtained with neural networks that use dropout without changing anything during training. The only difference comes at prediction time in which instead of doing a single forward pass and multiply each layer by a weight proportional to dropout probability, several forward passes with dropout must be done instead. They also showed in \cite{gal2015modern} that stochastic regularization techniques in arbitrary neural models can be seen as approximate variational inference in Bayesian Neural Networks.

This work is further extended by the same authors and they showed that the same ideas of dropout as a Bayesian approximation can be used in CNNs \cite{gal2015bayesian}. In particular, they showed that dropout can be seen as approximate variational inference in Bayesian Neural Networks, thus permitting the use of operations such as convolution and pooling in probabilistic models. The implementation is a matter of just performing dropout after each convolution layer at training, and by performing several stochastic forward passes through the model. The CNN model is then used in an Active Learning environment, where the goal is to label images intelligently so that a model has good performance with fewer training examples \cite{Gal2016Active}. \citeauthor{Gal2016Active} \cite{Gal2016Active} are able to achieve 5\% test error on the MNIST data set \cite{lecun1998gradient} with only 295 labeled images, and 1.64\% test error with 1000 labeled images. They compare different acquisition functions and also compare the Bayesian paradigm with frequentist CNNs.

The dropout variational approach can also be used in Recurrent Neural Networks (RNNs), as shown by \citeauthor{gal2016theoretically} in \cite{gal2016theoretically}. In this paper, the authors give insight on how to use dropout with RNNs and apply it on Long Short-Term Memory (LSTM) and Gated recurrent unit (GRU) models, outperforming existing techniques in language modeling with the Penn Treebank data set.

An example of how the uncertainty provided by Bayesian Neural Networks can be used is shown by \citeauthor{li2017dropout} \cite{li2017dropout}. They use adversarial examples and check if the original image can be told apart from the modified one by examining the uncertainty representation of the Bayesian models. The deterministic Neural Networks predict the wrong label very confidently on these adversarial samples. Dropout models, while also predicting wrong labels, are uncertain about their predictions. They finish by stating that their results suggest that assessing the uncertainty of classification models can be used to identify adversarial examples, but much more research is needed to solve the difficulties faced with adversarial inputs.

% The research on adversarial examples is continued by \citeauthor{rawat2017adversarial}, who study different Bayesian approaches (Bayes by Backprop (BBB), Probabilistic Backpropagation (PBP), Variational Matrix Gaussian (VMG) and MC-Dropout) and the uncertainty provided by the models, and prove that these models exhibit increased uncertainty when under attack \cite{rawat2017adversarial}. They state that all of the architectures in their study are Multi-Layer Perceptron, which do not scale for high dimensional colored images, with the exception of MC-Dropout, and therefore, a detailed study for MC-Dropout is required to demonstrate the applicability of model uncertainty for adversarial detection on various state-of-the-art attacks.

In this work, the Bayesian approximation using dropout will be used and compared with conventional methods in the context of Active Learning, following and extending the work in \cite{Gal2016Active}. First, the original paper is reproduced by training the exact same architecture and using the same acquisition functions in the same data set. Then, these ideas are tested in two different and more complex data sets.

The rest of this dissertation is organized as follows. In Chapter \ref{ch:machine_learning}, a brief overview on Machine Learning, and statistical inference is provided. This will help clarify the main differences between maximum likelihood estimation and uncertainty updating for the unknown parameters of the model. In particular, posterior sampling is useful to propagate parameter uncertainty into informative posterior predictive distributions. Chapter \ref{ch:variational_inference} studies the concept of Bayesian variational approximation to posterior distributions. In Chapter \ref{ch:ann} the basic concepts of neural networks are introduced, and then extended to include the theory of convolutional neural networks. In Chapter \ref{ch:active_learning}, the concept of active learning is studied and the acquisition functions used in this work are introduced. Concluding remarks are provided in Chapter \ref{ch:conclusions}.
