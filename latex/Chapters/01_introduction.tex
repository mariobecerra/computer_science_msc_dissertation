%!TEX root = ../msc_thesis.tex

\chapter{Introduction}
\label{ch:intro}

% 1) Introduction to the introduction: The first step will be a short version of the three moves, often in as little as three paragraphs, ending with some sort of transition to the next section where the full context will be provided.
% 2) Context: Here the writer can give the full context in a way that flows from what has been said in the opening. The extent of the context given here will depend on what follows the introduction; if there will be a full lit review or a full context chapter to come, the detail provided here will, of course, be less extensive. If, on the other hand, the next step after the introduction will be a discussion of method, the work of contextualizing will have to be completed in its entirely here.
% 3) Restatement of the problem: With this more fulsome treatment of context in mind, the reader is ready to hear a restatement of the problem and significance; this statement will echo what was said in the opening, but will have much more resonance for the reader who now has a deeper understanding of the research context.
% 4) Restatement of the response: Similarly, the response can be restated in more meaningful detail for the reader who now has a better understanding of the problem.
% 5) Roadmap: Brief indication of how the thesis will proceed.


The main goal of this work is to compare Bayesian methods in Deep Learning with traditional methods in the context of Active Learning. Traditional methods are based on the optimization of a loss function, which is rooted in maximum likelihood estimation and only offer a single point estimate with no measure of uncertainty in the prediction or the parameters, whereas the Bayesian approach gives a full posterior distribution for each of the parameters, thus giving as well a posterior predictive distribution of the response variable. Bayesian methods avoid overfitting as they average over parameter values.

% Bayesian methods for Artificial Neural Networks date back to the beginning of the 90s with Radford Neal's PhD thesis \cite{neal1996bayesian}, some work by \citeauthor{denker1991transforming} \cite{denker1991transforming} and some by \citeauthor{mackay1992practical} \cite{mackay1992practical}. In particular, \citeauthor{neal1996bayesian} used Hamiltonian Monte Carlo for posterior inferences of the parameters \cite{neal1996bayesian}. These approaches are not usually used because they are slow to train and do not scale well to the big networks and data sets used nowadays. With the work of \citeauthor{gal2016uncertainty}, Bayesian approaches can be used for Deep Learning without making major changes to learning algorithms, this is because it has been proven that using dropout during training and at prediction time is equivalent to approximating the posterior distribution of a Bayesian Neural Network \cite{gal2016uncertainty}.
%
% In this work, the Bayesian approach using dropout will be used and compared with traditional methods in the context of Active Learning, which aims to reduce the number of tagged examples for a supervised learning problem, as done in \cite{Gal2016Active}.
%
% \section{Related work}

%These stochastic forward passes are referred to as Monte Carlo (MC) dropout. Dropout is not usually performed after convolutional layers because it does not seem to give any benefits, but \citeauthor{gal2015bayesian} show empirically that MC-dropout can help with overfitting.


% The work on CNNs is then used in an Active Learning environment, where the goal is to label images intelligently so that a model has good performance with fewer training examples \cite{Gal2016Active}. Deep learning poses several difficulties when used in an Active Learning setting because, first, small amounts of data need to be handled; and second, many Active Learning acquisition functions rely on model uncertainty, which is rarely represented in Deep Learning. \citeauthor{Gal2016Active} are able to achieve 5\% test error on the MNIST data set with only 295 labeled images, and 1.64\% test error with 1000 labeled images. They compare five different acquisition functions: choosing images that maximize the predictive entropy (Max Entropy); the ones that maximize the mutual information between predictions and model posterior (BALD); the ones that maximize the Variation Ratios; the ones that maximize mean standard deviation; and random acquisition (baseline). They also compare it with deterministic CNNs which, like the Bayesian CNN, produce a probability vector which can be used with the acquisition functions, but the Bayesian models, propagating uncertainty throughout the model, attain higher accuracy early on, and converge to a higher accuracy overall.

Bayesian methods for Artificial Neural Networks to late 1980s and early 1990s. These first approaches focused on Markov Chain Monte Carlo (MCMC) as the main method get samples of the posterior distribution of the parameters, such as \citeauthor{neal1996bayesian} in his PhD thesis \cite{neal1996bayesian}, some work by \citeauthor{denker1991transforming} \cite{denker1991transforming} and some by \citeauthor{mackay1992practical} \cite{mackay1992practical}. These approaches are not usually used because they are slow to train and do not scale well to the big networks and data sets used nowadays. Lately there has been more work in the area, such as the Probabilistic Backpropagation algorithm developed by \citeauthor{hernandez2015probabilistic}, which relies on one-dimensional Gaussian distributions that approximate the marginal posterior distribution of the weights on each iteration of backpropagation \cite{hernandez2015probabilistic}. Another example is the work done by \citeauthor{graves2011practical}, based on variational inference to approximate the posterior distribution of the parameters \cite{graves2011practical}. However, these methods do not scale well to very big network architectures and data sets; and they also have the disadvantage that they only work with multi-layer perceptron architectures, making them impossible to use with more recent architectures such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs).

Very recently, \citeauthor{gal2015dropout1} showed that a neural network with arbitrary depth, with dropout applied before each weight layer, is equivalent to the variational approximation to a deep Gaussian process. This is because the loss function minimizes the Kullback Leibler divergence between an approximate distribution and the posterior of a deep Gaussian process \cite{gal2015dropout1}. This means that we can get uncertainty estimates with the models already trained with dropout without changing anything during training, the only difference comes at prediction time in which instead of doing a single forward pass and multiply each layer by a weight proportional to dropout probability, we just do several forward passes with dropout. They also showed in \cite{gal2015modern} that stochastic regularization techniques in arbitrary neural models can be seen as approximate variational inference in Bayesian Neural Networks.

This work is further extended by the same authors and they showed that the same ideas of dropout as a Bayesian approximation can be used in CNNs \cite{gal2015bayesian}. In particular, the showed that dropout can be seen as approximate variational inference in Bayesian Neural Networks, thus permitting the use of operations such as convolution and pooling in probabilistic models. The implementation is reduced to performing dropout after each convolution layer at training, and by performing several stochastic forward passes through the model. The work on CNNs is then used in an Active Learning environment, where the goal is to label images intelligently so that a model has good performance with fewer training examples \cite{Gal2016Active}. \citeauthor{Gal2016Active} are able to achieve 5\% test error on the MNIST data set with only 295 labeled images, and 1.64\% test error with 1000 labeled images. They compare different acquisition functions and also compare the Bayesian paradigm with deterministic CNNs.

The dropout variational approach can also be used in Recurrent Neural Networks (RNNs), as shown by \citeauthor{gal2016theoretically} in \cite{gal2016theoretically}. In this paper, the authors give insight on how to use dropout with RNNs, something seldom done, and apply it on LSTM and GRU models, outperforming existing techniques in language modeling with the Penn Treebank data set.

An example of how the uncertainties provided by Bayesian Neural Networks can be used, is shown by \citeauthor{li2017dropout} in \cite{li2017dropout}. They use adversarial examples and check if they can be told apart by examining the uncertainty representation of the dropout models. The deterministic Neural Networks produce over-confident predictions on these adversarial samples (they predict the wrong label very confidently), while dropout models, though producing wrong labels, are very uncertain about their predictions. They finish by stating that their results suggest that assessing the uncertainty of classification models can be used to identify adversarial examples, but much more research is needed to solve the difficulties faced with adversarial inputs.

The research on adversarial examples is continued by \citeauthor{rawat2017adversarial}, who study different Bayesian approaches (Bayes by Backprop (BBB), Probabilistic Backpropagation (PBP), Variational Matrix Gaussian (VMG) and MC-Dropout) and the uncertainty provided by the models, and prove that these models exhibit increased uncertainty when under attack \cite{rawat2017adversarial}. They state that all of the architectures in their study are Multi-Layer Perceptron, which do not scale for high dimensional colored images, with the exception of MC-Dropout, and therefore, a detailed study for MC-Dropout is required to demonstrate the applicability of model uncertainty for adversarial detection on various state-of-the-art attacks.

In this work, the Bayesian approach using dropout will be used and compared with traditional methods in the context of Active Learning as done in \cite{Gal2016Active}. We first reproduce the original paper by training the exact same architecture and using the same acquisition functions in the same data set. Then, these ideas are tested in two different and more complex data sets.

In chapter \ref{ch:theory}, the theory needed to understand the experiments is studied, including machine learning, neural networks, Bayesian theory, and variational inference. Chapter \ref{ch:results} shows the results of the implementations of the models in the three data sets used. And, finally, chapter \ref{ch:conclusions} we present our conclusions and potential future work.
