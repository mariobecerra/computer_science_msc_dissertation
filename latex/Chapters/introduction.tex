%!TEX root = ../msc_thesis.tex

\chapter{Introduction}
\label{ch:intro}

% 1) Introduction to the introduction: The first step will be a short version of the three moves, often in as little as three paragraphs, ending with some sort of transition to the next section where the full context will be provided.
% 2) Context: Here the writer can give the full context in a way that flows from what has been said in the opening. The extent of the context given here will depend on what follows the introduction; if there will be a full lit review or a full context chapter to come, the detail provided here will, of course, be less extensive. If, on the other hand, the next step after the introduction will be a discussion of method, the work of contextualizing will have to be completed in its entirely here.
% 3) Restatement of the problem: With this more fulsome treatment of context in mind, the reader is ready to hear a restatement of the problem and significance; this statement will echo what was said in the opening, but will have much more resonance for the reader who now has a deeper understanding of the research context.
% 4) Restatement of the response: Similarly, the response can be restated in more meaningful detail for the reader who now has a better understanding of the problem.
% 5) Roadmap: Brief indication of how the thesis will proceed.


The main goal of this work is to compare Bayesian methods with traditional methods used in Deep Learning within an Active Learning context. Traditional methods are based on the optimization of a loss function, which is rooted in maximum likelihood estimation. These methods provide only a single point estimate with no measure of uncertainty in the prediction or the parameters. On the other hand, the Bayesian approach gives a joint posterior distribution of the parameters, thus giving a posterior predictive distribution of the response variable as well. Bayesian methods avoid overfitting as they average over parameter values. Bayesian methods in Deep Learning are seldom used in literature, in contrast with traditional methods which are widely used.

Bayesian methods for Artificial Neural Networks date back to the late 1980s and early 1990s. These first approaches focused on Markov Chain Monte Carlo (MCMC) as the main method get samples of the posterior distribution of the parameters, such as \citeauthor{neal1996bayesian} \cite{neal1996bayesian}, \citeauthor{denker1991transforming} \cite{denker1991transforming} and \citeauthor{mackay1992practical} \cite{mackay1992practical}. These approaches are not usually used because they are slow to train and do not scale well to the big Artificial Neural Networks and data sets used nowadays. Lately there has been more work in the area, such as the Probabilistic Backpropagation algorithm developed by \citeauthor{hernandez2015probabilistic} \cite{hernandez2015probabilistic}. This algorithm relies on one-dimensional Gaussian distributions that approximate the marginal posterior distribution of the weights on each iteration of backpropagation . Another example is the work done by \citeauthor{graves2011practical} \cite{graves2011practical}, based on variational inference to approximate the posterior distribution of the parameters. However, these methods do not scale well to very big neural network architectures and data sets. They also have the disadvantage of only working with multi-layer perceptron architectures, making them impossible to use with more recent architectures such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs).

Very recently, \citeauthor{gal2015dropout1} \cite{gal2015dropout1} showed that a neural network with arbitrary depth, with dropout applied before each weight layer, is equivalent to the variational approximation to a deep Gaussian process. Dropout is the name of a widely used stochastic regularization technique, which is discussed later in this work. The equivalence between a neural network trained with dropout and a deep Gaussian process comes from the fact that the loss function minimizes the Kullback Leibler divergence between an approximate distribution and the posterior of a deep Gaussian process. This means that uncertainty estimates can be obtained with the models that have been trained with dropout without changing anything during training. The only difference comes at prediction time in which instead of doing a single forward pass and multiply each layer by a weight proportional to dropout probability, several forward passes with dropout must be done instead. They also showed in \cite{gal2015modern} that stochastic regularization techniques in arbitrary neural models can be seen as approximate variational inference in Bayesian Neural Networks.

This work is further extended by the same authors and they showed that the same ideas of dropout as a Bayesian approximation can be used in CNNs \cite{gal2015bayesian}. In particular, they showed that dropout can be seen as approximate variational inference in Bayesian Neural Networks, thus permitting the use of operations such as convolution and pooling in probabilistic models. The implementation is reduced to performing dropout after each convolution layer at training, and by performing several stochastic forward passes through the model. The work on CNNs is then used in an Active Learning environment, where the goal is to label images intelligently so that a model has good performance with fewer training examples \cite{Gal2016Active}. \citeauthor{Gal2016Active} are able to achieve 5\% test error on the MNIST data set with only 295 labeled images, and 1.64\% test error with 1000 labeled images. They compare different acquisition functions and also compare the Bayesian paradigm with deterministic CNNs.

The dropout variational approach can also be used in Recurrent Neural Networks (RNNs), as shown by \citeauthor{gal2016theoretically} in \cite{gal2016theoretically}. In this paper, the authors give insight on how to use dropout with RNNs, something seldom done, and apply it on LSTM and GRU models, outperforming existing techniques in language modeling with the Penn Treebank data set.

An example of how the uncertainties provided by Bayesian Neural Networks can be used, is shown by \citeauthor{li2017dropout} in \cite{li2017dropout}. They use adversarial examples and check if they can be told apart by examining the uncertainty representation of the dropout models. The deterministic Neural Networks produce over-confident predictions on these adversarial samples (they predict the wrong label very confidently), while dropout models, though producing wrong labels, are very uncertain about their predictions. They finish by stating that their results suggest that assessing the uncertainty of classification models can be used to identify adversarial examples, but much more research is needed to solve the difficulties faced with adversarial inputs.

The research on adversarial examples is continued by \citeauthor{rawat2017adversarial}, who study different Bayesian approaches (Bayes by Backprop (BBB), Probabilistic Backpropagation (PBP), Variational Matrix Gaussian (VMG) and MC-Dropout) and the uncertainty provided by the models, and prove that these models exhibit increased uncertainty when under attack \cite{rawat2017adversarial}. They state that all of the architectures in their study are Multi-Layer Perceptron, which do not scale for high dimensional colored images, with the exception of MC-Dropout, and therefore, a detailed study for MC-Dropout is required to demonstrate the applicability of model uncertainty for adversarial detection on various state-of-the-art attacks.

In this work, the Bayesian approach using dropout will be used and compared with traditional methods in the context of Active Learning as done in \cite{Gal2016Active}. We first reproduce the original paper by training the exact same architecture and using the same acquisition functions in the same data set. Then, these ideas are tested in two different and more complex data sets.

The rest of this document is organized as follows. In chapter \ref{ch:machine_learning}, the concepts of machine learning, frequentist and Bayesian statistics are introduced, including concepts such as maximum likelihood estimation, loss function, posterior distribution and posterior predictive distribution. Chapter \ref{ch:variational_inference} studies the concept of Bayesian variational approximation to posterior distributions. In chapter \ref{ch:ann} the basic concepts of neural networks are introduced, and then extended to include the theory of convolutional neural networks. In chapter \ref{ch:active_learning}, the concept of active learning is studied and the acquisition functions used in this work are introduced. And, finally, chapter \ref{ch:conclusions} presents conclusions and potential future work.
