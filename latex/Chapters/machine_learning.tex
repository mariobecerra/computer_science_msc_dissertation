%!TEX root = ../msc_thesis.tex

\chapter{Machine learning}
\label{ch:machine_learning}

\section{Machine learning}

Machine learning is a research field that focuses on learning patterns lurking within a dataset. Such decisions include making predictions about new unseen data, clustering, or have an agent taking actions based on a reward signal \cite{murphy2012machine, sutton1998reinforcement}. Machine learning has seen an increasing interest from the scientific and industry thanks to the advance of computing abilities. Such computing abilities have made possible most modern techniques. Computer power has been increasing exponentially since the 1960s, and this growth is known as Moore's law. It was first stated in 1965 \cite{moore2006cramming, moore1975progress}, claiming that the number of transistors in an integrated computer circuit doubles about every two years for half the cost. Moore's law has helped the development of faster computers, as well as making these computers common in all areas of everyday life. This has led, in turn, to an exponential increase in data availability which has resulted in increased data storage requirements. However, Moore's law is said to be at its limit nowadays. Meaning that it has been increasingly difficult to double the number of transistors in that two year time frame while also being inexpensive. This has led to different research directions in the field of computer architecture to find new ways in which computing power can be enhanced without sacrificing costs. The limit of Moore's law is an important factor in machine learning. The reason is, that is not enough to harness faster and better computing resources, but also to use them efficiently.

Machine learning is usually divided in supervised learning, unsupervised learning and reinforcement learning. The first involves learning a model that relates a response to a set of possible explanatory features. The second involves learning relationships and the structure of the data. The third involves learning a strategy for a decision maker in order to maximize their reward while interacting with an unknown environment. Reinforcement learning (RL) is similar to supervised learning. Nonetheless, the difference is that in the framework of RL, the learner is continually interacting with its environment. In this setting, the agent receives feedback after every action is taken. Whereas, in supervised learning the agent is expected to  receive a dataset from an oracle which, hopefully, includes all the right answers. That is, in supervised learning there is an oracle that tells the learner the right answer, whereas in RL the oracle only tells it how to perform relative to past actions.

For example, in supervised learning one would like to estimate the income of a household given a dataset that includes a set of relevant predictors, such as zip code, home size, number of owned cars, etc.
An example of unsupervised learning would be to use the information in the dataset to be able to group together similar households in terms of such predictors. This will hopefully uncover a natural structure in the dataset, where households will be grouped with respect to its characteristics. In this setting, it is expected that different groups will be qualitatively different from one another.
An example of RL would be to have an agent learning how to invest in real estate; observing its rewards after the investment period.
Recently, supervised learning and reinforcement learning have been combined to achieve harder goals for computers, such as beating the Go world champion \cite{silver2017mastering}.
In this work, supervised learning will be discussed in more detail. Eventually, in Chapter \ref{ch:active_learning} supervised learning will be merged with unsupervised learning, giving rise to the main application of this thesis. That is, active learning, a mix of using known structured data to uncover patterns from non-structured data.

As mentioned above, in supervised learning there is a response associated to a set of relevant features. In this setting, let $\boldsymbol{y}$ denote the response
% $n$ elements, and each feature is denoted by $\boldsymbol{x}^{(k)} \in \mathbb{R}^n$ for $k$ from 1 to $p$. All features can be summarized in notation in a single data matrix $\boldsymbol{X} \in \mathbb{R}^{n \times p}$. It is assumed that there exists a relationship between $\boldsymbol{y}$ and $\boldsymbol{X}$, which can be written as
and $\boldsymbol{x} \in \mathbb{R}^p$ denote a vector containing the features available to the analyst. The dataset consists of $n$ observations of such response stored in $\boldsymbol{y} \in \mathbb{R}^n$.
The corresponding features are stored in a matrix $\boldsymbol{X} \in \mathbb{R}^{n \times p}$. Consequently, each feature will be denoted by column vectors $\boldsymbol{x}^{(k)} \in \mathbb{R}^n$ for $k \in \left\{1, \hdots, p \right\}$, and each datapoint will be represented as a row vector $\boldsymbol{x}_i \in \mathbb{R}^p$ for $i \in \left\{1, \hdots, n \right\}$.
It is assumed that there exists a relationship between $\boldsymbol{y}$ and $\boldsymbol{X}$, which can be written as
\begin{equation}
  \label{eq:general_learning_model}
  \boldsymbol{y} = f(\boldsymbol{X}) + \boldsymbol{\varepsilon},
\end{equation}
where $f$ is a fixed but unknown function and $\boldsymbol{\varepsilon}$ is a zero-mean random error term, independent of $\boldsymbol{X}$ \cite[p.~16]{james2013introduction}.
The goal is to find the best approximation to the function $f$, which is done with an estimate $\hat{f}$ \cite[p.~17]{james2013introduction}.
This estimate $\hat{f}$ is chosen from a family of functions $\mathcal{F}$. In machine learning $\mathcal{F}$ is known as the hypothesis set.

For practical convenience, the family $\mathcal{F}$ is assumed to be indexed by a finite dimensional vector $\boldsymbol{\theta}$. Thus allowing to think of $\mathcal{F}_{\boldsymbol{\theta}}$ as a family of approximations indexed by a finite set of unknown parameters. This changes the task of finding a candidate in an infinite dimensional space to a problem in a finite dimensional space such as $\mathbb{R}^d$.
In general, it is desirable for $\hat{f}$ to keep the generalization error to a minimum, where the generalization error is defined by the error of predicting for any unseen vector $\boldsymbol{x}^*$, with $\boldsymbol{x}^*$ randomly sampled from the probability distribution of features. In other terms, the error is usually expressed as the cost (loss) of making a wrong prediction.
Of course, the loss function for the generalization error is not known, as it depends on unseen data. However, one minimizes the empirical loss function defined by the set of training data available. That is, the empirical loss function is approximated using the available dataset of $n$ observations $\mathcal{D} = \left\{ (\boldsymbol{X}, \boldsymbol{y}) \right\}$, where $\boldsymbol{X}$ and $\boldsymbol{y}$ have been defined above.
% This loss function needs a set of training data $\left\{ (\boldsymbol{x_1}, y_1), ..., (\boldsymbol{x_n}, y_n) \right\}$, where each $y_i$ and $\boldsymbol{x_i}$ denote the $i$-th entry of $\boldsymbol{y}$ and $i$-th row of $\boldsymbol{X}$, respectively, for $i \in \left\{1, \ldots, n \right\}$.
Examples of $\mathcal{F}_{\boldsymbol{\theta}}$ are the linear regression model, the general additive model, the logistic regression model and the neural network.

% For example, in the linear regression case, it is common to minimize the squared error function
For example, a linear regression can be assumed. The loss function can be defined as the quadratic error function defined as
\begin{equation}
  \hat{f} = \argmin_{f \in \mathcal{F}} \frac{1}{n} \sum_{i = 1}^n{ ( y_i - f(\boldsymbol{x_i}) ) ^ 2},
\end{equation}
where $\mathcal{F}_{\boldsymbol{\theta}}$ is the family of functions of the form $f_{\boldsymbol{\theta}}(\boldsymbol{X}) = \boldsymbol{X}\boldsymbol{\theta}$ with $\boldsymbol{\theta} \in \mathbb{R}^p$. Thus, the  estimate $\hat{f}$ is $\hat{f}(\boldsymbol{X}) = \boldsymbol{X} \hat{\boldsymbol{\theta}}$, where $\hat{\boldsymbol{\theta}}$ minimizes the error function.
In particular, the linear regression model belongs to a type of methods called \textbf{parametric methods}. In these methods, the modeler makes an assumption about the functional form of $f$, which depends on a vector of parameters $\boldsymbol{\theta}$ of fixed dimension. After this, the modeler proceeds to train the model by choosing the vector of parameters that minimizes a previously selected loss function \cite[p.~21]{james2013introduction}.
% In the case of linear regression, the form of $f$ is assumed to be $f(\boldsymbol{X}) = \boldsymbol{X}\boldsymbol{\theta}$, and the parameters chosen are the elements of the $\hat{\boldsymbol{\theta}}$ vector that minimize the squared error function of the model.

An approach to the estimation of parameters that is different to the concept of error function minimization is the one of \textbf{maximum likelihood estimation}.
This is a probabilistic approach in which a probabilistic model is assumed to the data generating mechanism. In the case of linear regression, it is commonly assumed that the error term in \eqref{eq:general_learning_model} has a Gaussian distribution such that
$\varepsilon_i \sim \normaldist{0}{\sigma^2}$,
then $y_i \sim \normaldist{\boldsymbol{x_i}^T \boldsymbol{\theta}}{\sigma^2}$ for $i \in \left\{ 1, \ldots, n \right\}$.
It is also usually assumed that the observations $(\boldsymbol{X}, \boldsymbol{y})$ come from a random sample and are independent of one another. Therefore, the log-likelihood of the sample is
\begin{equation}
  \label{eq:gaussian_likelihood}
  L(\boldsymbol{\theta}) = \sum_{i = 1}^n \log \left[ \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left({- \frac{(y_i - \boldsymbol{x_i}^T \boldsymbol{\theta})^2}{2\sigma^2}}\right) \right].
\end{equation}

%The \textbf{principle of maximum likelihood} states that the values for $\boldsymbol{\theta}$ should be chosen so that the observed probability of the random sample is the highest, i.e., so that they maximize equation \eqref{eq:gaussian_likelihood} \cite[p.~31]{friedman2001elements} \cite[p.~303]{roussas1973first}.

Maximum likelihood estimation uses the \textbf{likelihood principle}, which states that all the information given by a sample of data is entirely held within the likelihood function \cite{gelman2013bayesian} \cite{robert2007bayesian}. Maximum likelihood estimation is just an implementation of the likelihood principle which consists in choosing the values for $\boldsymbol{\theta}$ so that they maximize the likelihood of the data, i.e., equation \eqref{eq:gaussian_likelihood} \cite{robert2007bayesian}. Informally, this is choosing the values that maximize the probability of observing the given data \cite[p.~31]{friedman2001elements} \cite{robert2007bayesian}. With some algebra, it is fairly easy to see that maximizing equation \eqref{eq:gaussian_likelihood} is equivalent to minimizing the squared error loss.

%The \textbf{principle of maximum likelihood} states that the values for $\boldsymbol{\theta}$ should be chosen so that they maximize the likelihood of the data, i.e., equation \eqref{eq:gaussian_likelihood} \cite[p.~31]{friedman2001elements} \cite[p.~303]{roussas1973first}. With some algebra, it is fairly easy to see that maximizing equation \eqref{eq:gaussian_likelihood} is equivalent to minimizing the squared error loss.

%The \textbf{principle of maximum likelihood} states that the values for $\boldsymbol{\theta}$ should be chosen so that the data of the random sample is most likely, i.e., so that they maximize equation \eqref{eq:gaussian_likelihood} \cite[p.~31]{friedman2001elements} \cite[p.~303]{roussas1973first}. With some algebra, it is fairly easy to see that maximizing equation \eqref{eq:gaussian_likelihood} is equivalent to minimizing the squared error loss.

An alternative setting is that of classification. In this case, it is assumed that $y_i$ follows a Bernoulli distribution, such that the probability of $y_i$ of being 1 is $p_i(\boldsymbol{\theta})$, where $\boldsymbol{\theta}$ is the parameter that indexes the probability. Then, the likelihood function in this case is $\prod_{i = 1}^n  p_i(\boldsymbol{\theta})^{y_i}\left(1 - p_i(\boldsymbol{\theta}) \right)^{1 - y_i}$ and thus, the log-likelihood is
\begin{equation}
  L(\boldsymbol{\theta}) = \sum_{i = 1}^n \left[ y_i \log\left( p_i(\boldsymbol{\theta}) \right) + (1 - y_i) \log \left( 1 - p_i(\boldsymbol{\theta}) \right) \right].
\end{equation}

An alternative to maximum likelihood estimation is given by the \textbf{Bayesian approach}. In this setting, the uncertainty about the unknown parameters is stated as a probability distribution.
First, a prior distribution on the parameters $\boldsymbol{\theta}$ is assumed, and then the knowledge about them is updated with data. That is, the modeler first uses a prior distribution $\prob{\boldsymbol{\theta}}$ to quantify the knowledge that they may have about $\boldsymbol{\theta}$, and then computes the posterior distribution of $\boldsymbol{\theta}$ given $\boldsymbol{X}$ and $\boldsymbol{y}$ using Bayes' theorem as such
\begin{equation}
  \label{eq:bayes_theorem}
  \prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X} } =
  \frac{\prob{\boldsymbol{y} | \boldsymbol{\theta}, \boldsymbol{X}} \prob{\boldsymbol{\theta} | \boldsymbol{X}}}{\prob{\boldsymbol{y} | \boldsymbol{X}}} =
  \frac{\prob{\boldsymbol{y} | \boldsymbol{\theta}, \boldsymbol{X}} \prob{\boldsymbol{\theta} | \boldsymbol{X}}}{\int \prob{\boldsymbol{y} | \boldsymbol{\theta}, \boldsymbol{X}} \prob{\boldsymbol{\theta} | \boldsymbol{X}} d\boldsymbol{\theta}}.
\end{equation}

The posterior distribution represents the knowledge about $\boldsymbol{\theta}$ after the data has been observed. It is a compromise between the prior beliefs and the data. Note that the likelihood of the data $\prob{\boldsymbol{y} | \boldsymbol{\theta}, \boldsymbol{X}}$ is used in the numerator of equation \eqref{eq:bayes_theorem}. In fact, the Bayesian paradigm permits another implementation of the likelihood principle, just like maximum likelihood, but with the added benefit of including decision-related requirements \cite{robert2007bayesian}.

% An advantage of the Bayesian approach is that uncertainty can be mathematically described by using probability.
An advantage of the Bayesian approach is that it uses the language of probability to mathematically describe the uncertainty on unknown parameters and other components in the modeling process.
In addition, Bayesian inference is logically consistent while dealing with this uncertainty \cite{cox1946probability} \cite{cox1963algebra} \cite{jaynes2003probability} \cite{o2004advanced}. A philosophical difference between the frequentist and the Bayesian approach is that in the latter, the degree of belief of the researcher is represented in the distribution of the parameters and hence, they are treated as random variables. Whereas in the former, they are treated as a fixed but unknown value \cite{o2004advanced}.
% The multiplier $\prob{\boldsymbol{y} | \boldsymbol{\theta}, \boldsymbol{X}}$ is called the likelihood, and is the probability of the data given the parameters.

Since the denominator of equation \eqref{eq:bayes_theorem} does not depend on $\boldsymbol{\theta}$, as it is only a normalizing constant, it is just usually described as a proportion
\begin{equation}
  \label{eq:bayes_theorem_prop}
    \prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}} \propto \prob{\boldsymbol{y} | \boldsymbol{\theta}, \boldsymbol{X}} \prob{\boldsymbol{\theta} | \boldsymbol{X}}.
\end{equation}

The posterior distribution is also used to predict the values of unobserved data. Let $\boldsymbol{x}^*$ be a vector of features for which a prediction of $\hat{\boldsymbol{y}}^*$ is desired, then the \textbf{posterior predictive distribution} must be used, defined as
\begin{equation}
  \label{eq:post_pred_dist}
  \prob{\boldsymbol{y}^* | \boldsymbol{X}, \boldsymbol{y}, \boldsymbol{x}^*} = \int \prob{\boldsymbol{y}^* | \boldsymbol{\theta}, \boldsymbol{x}^*} \prob{\boldsymbol{\theta} | \boldsymbol{X}, \boldsymbol{y}} d\boldsymbol{\theta}.
\end{equation}

Note that this posterior distribution is, as it name implies, a distribution, i.e., it is not a single prediction but infinitely many predictions, weighted by how probable each value is to be.
If the goal is to have a point prediction $\hat{\boldsymbol{y}}^*$ for $\boldsymbol{x}^*$, then the modeler could use Decision Theory to find a point estimate. The idea is to specify a loss function $L(\boldsymbol{y}^*, \hat{\boldsymbol{y}}^*)$ that quantifies the loss of having an estimate $\hat{\boldsymbol{y}}^*$ when the real value is $\boldsymbol{y}^*$, and have the point estimate be the $\hat{\boldsymbol{y}}^*$ that minimizes this loss function.
The optimal value for the squared loss $L(\boldsymbol{y}^*, \hat{\boldsymbol{y}}^*) = (\boldsymbol{y}^* - \hat{\boldsymbol{y}}^*)^2$ is the expected value of the posterior predictive distribution, that is,
\begin{equation}
  \hat{\boldsymbol{y}}^* = \mathbb{E}_{\boldsymbol{y}^* | \boldsymbol{x}^*, \boldsymbol{y}, \boldsymbol{X}} \left[ \boldsymbol{y} \right].
\end{equation}
% \begin{equation}
%   \hat{\boldsymbol{y}}^* = \mathbb{E}_{\boldsymbol{y}^* | \boldsymbol{x}^*, \boldsymbol{y}, \boldsymbol{X}} \left[ \prob{\boldsymbol{y}^* | \boldsymbol{X}, \boldsymbol{y}, \boldsymbol{x}^*} \right].
% \end{equation}

Another loss function is the absolute loss $L(\boldsymbol{y}^*, \hat{\boldsymbol{y}}^*) = \vert \boldsymbol{y}^* - \hat{\boldsymbol{y}}^* \vert$ which results in using the median of the posterior predictive distribution as a point estimate. Finally, the 0-1 loss function $L(\boldsymbol{y}^*, \hat{\boldsymbol{y}}^*) = \mathbb{I}(\boldsymbol{y}^* \neq \hat{\boldsymbol{y}}^* )$ results in using the mode of the posterior predictive distribution as a point estimate. This last estimate is also called a maximum a posteriori (MAP) estimate. It should be noted that under certain conditions the MAP estimate is equivalent to the MLE.

\subsection{Example}

All these concepts can be illustrated in the context of logistic regression. Suppose that we have $n$ observations of a binary response variable $\boldsymbol{y} \in \left\{0, 1\right\}^n$ and two continuous features $\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)} \in \mathbb{R}^n$. The frequentist approach will be studied first.
Then, it will stated as loss function minimization and eventually as an inference problem in the Bayesian framework.
% then it will be transformed into a simple learning problem in which the goal is to minimize a loss function, and finally, the Bayesian approach will be examined.

If someone wanted to build a classifier using a linear regression model, then they would run into a problem because a linear model of the form $\mu(\boldsymbol{x_i}) = \theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)}$ is not bounded. In contrast, the response variable $\boldsymbol{y}$ from the data only takes the values $0$ and $1$, hence, it is a better idea to model the probability that a feature vector belongs to each class. This way, $y_i$ is a random variable that follows a Bernoulli distribution with parameter $p_i$, where $p_i$ is the probability of $y_i$ being 1.
The range of $f$ as a linear model is $\mathbb{R}$, so it must be mapped to the $\left[ 0,1 \right]$ interval in which probabilities live, and it can be done with what is called a sigmoid function. The \textbf{logistic sigmoid function} % $\sigma \left( \cdot \right)$
is a widely used link function for this type of problems \cite[p.~114]{christopher2006pattern}. It is defined as
\begin{equation}
  \sigma(w) = \frac{e^w}{1 + e^w} = \frac{1}{1 + e^{-w}}.
\end{equation}
It is easy to see that $\lim_{w \to -\infty} \sigma(w) = 0$ and $\lim_{w \to \infty} \sigma(w) = 1$.

%There is more reasoning behind the logistic sigmoid function, and some of it has to be that it belongs to the exponential family of functions. For more details about this, see \cite{christopher2006pattern}.

Another possible sigmoid function is the \textbf{probit function}, defined as the inverse of the cumulative distribution function of a Gaussian random variable \cite[p.~296]{friedman2001elements}. That is, $\sigma(w) = \Phi^{-1}\left( x \right)$,
where
\begin{equation}
  %\Phi(w) = \int_{-\infty}^w \frac{1}{\sqrt{2 \pi}} \exp{\left( -\frac{x^2}{2} \right)} dx = \int_{-\infty}^w \normalfunc{x}{0}{1} dx.
  \Phi(w) = \int_{-\infty}^w \frac{1}{\sqrt{2 \pi}} \exp{\left( -\frac{x^2}{2} \right)} dx.
\end{equation}

There are some differences in the theory and behavior of the logistic and the probit functions, but in practice they hardly make any substantial difference and the choice of one over the other may be a matter of taste or convenience \cite[p.~118]{gelman2006data}. Henceforth, we will use the logistic function as the sigmoid function for binary classification problems.

It has been established that $\sigma \left( \cdot \right)$ maps from $\mathbb{R}$ to $\left[ 0,1 \right]$, but it is yet to be explained how to relate this to the original problem which is to build a linear classifier using the vectors $\boldsymbol{x}^{(1)}$ and $\boldsymbol{x}^{(2)}$. Using  $\sigma \left( \cdot \right)$, one can define the probability that $y_i$ belongs to class $1$ given features $x_i^{(1)}$ and $x_i^{(2)}$ as
\begin{equation}
  \prob{y_i = 1 | \boldsymbol{x_i}, \boldsymbol{\theta}} = \sigma(\theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)}) = \frac{1}{1 + e^{-\left( \theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)} \right)}},
\end{equation}
with $\boldsymbol{x_i} = \left[ x_i^{(1)}, x_i^{(2)} \right]^T$. Thus
\begin{equation}
  \prob{y_i = 0 | \boldsymbol{x_i}, \boldsymbol{\theta}} = 1 - \prob{y_i = 1 | \boldsymbol{x_i}} = \frac{1}{1 + e^{\theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)}}}.
\end{equation}

This means that $y_i$ is fully described as a Bernoulli distribution with parameter $p_i =\prob{y_i = 1 | \boldsymbol{x_i}, \boldsymbol{\theta}}$. When the response variable is Bernoulli distributed, the likelihood of $n$ independent observations is
\begin{equation}
  \prod_{i = 1}^n  p_i(\boldsymbol{\theta})^{y_i}\left(1 - p_i(\boldsymbol{\theta}) \right)^{1 - y_i}.
\end{equation}

Then, in consequence, the log-likelihood of $n$ independent observations is
\begin{equation}
  \label{eq:log_likelihood_logistic_example}
  L(\boldsymbol{\theta}) = \sum_{i = 1}^n \left[ y_i \log\left( p_i(\boldsymbol{\theta}) \right) + (1 - y_i) \log \left( 1 - p_i(\boldsymbol{\theta}) \right) \right],
\end{equation}
where $p_i(\boldsymbol{\theta}) = \sigma(\theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)})$.

The maximum likelihood estimator of $\boldsymbol{\theta} = \left[ \theta_0, \theta_1, \theta_2 \right]^T$ is the vector $\hat{\boldsymbol{\theta}}$ that maximizes $L(\boldsymbol{\theta})$ in equation \eqref{eq:log_likelihood_logistic_example}; that is $\hat{\boldsymbol{\theta}} = \argmax L(\boldsymbol{\theta})$.

Another way to interpret the problem of binary classification, is to consider the learning problem from an optimization perspective in which the goal is to minimize an appropriate loss function. It has been shown above that maximizing equation \eqref{eq:log_likelihood_logistic_example} leads to the maximum likelihood estimate. However, if the sign is changed, the result is a loss function that can be minimized, and it provides the exact same solution as before.

The new objective function that needs to be minimized is
\begin{equation}
  \label{eq:logistic_example_loss_function}
  L(\boldsymbol{\theta}) = - \sum_{i = 1}^n \left[ y_i \log\left( p_i(\boldsymbol{\theta}) \right) + (1 - y_i) \log \left( 1 - p_i(\boldsymbol{\theta}) \right) \right].
\end{equation}
This loss function is commonly known in the literature as the \textbf{binary entropy} loss.

Intuitively, this is an error function because if $y_i = 1$, then $1 - y_i = 0$, so the second term in the loss function vanishes and what is left remaining is $\log\left( p_i(\boldsymbol{\theta}) \right)$. Now, when $p_i(\boldsymbol{\theta})$ is big, that is, close to 1, then $\log\left( p_i(\boldsymbol{\theta}) \right) \to 0$. However, if $p_i(\boldsymbol{\theta})$ is small, that is, close to 0, then $\log\left( p_i(\boldsymbol{\theta}) \right) \to -\infty$, and so the overall loss function tends to infinity. This means that when the real value of $y_i$ is 1 and a low probability is assigned to it, then the loss function is large; but if a high probability is assigned to it, then the loss function is small. The same reasoning works for when $y_i = 0$. So, when there is an incorrect classification with very high probability, the loss function takes a large value.
% The same reasoning works for when $y_i = 0$: if a low probability is assigned, then the loss function is small; but if a high probability is assigned, then the loss function is large.
Hence, minimizing the loss function leads to choosing values of $\boldsymbol{\theta}$ that yield a high probability to the cases when $y_i = 1$ and a low one when $y_i = 0$.

% There are other possible loss functions for this problem, such as the \textbf{hinge loss}

The logistic regression model can be reformulated from a Bayesian approach. As before, it will be assumed that $y_i$ follows a Bernoulli distribution such that the probability of being 1 is $p_i(\boldsymbol{\theta}) = \sigma(\theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)})$, where $\sigma \left( \cdot \right)$ is the logistic sigmoid function. In the Bayesian paradigm, the unknown parameter vector $\boldsymbol{\boldsymbol{\theta}} = \left[ \theta_0, \theta_1, \theta_2 \right]^T$ must have a joint prior distribution. A good idea is to assume a Gaussian distribution for the unknown unobservable parameter $\boldsymbol{\boldsymbol{\theta}}$. Furthermore, independence between the components of $\boldsymbol{\boldsymbol{\theta}}$ can be assumed which in turn allows one to write the prior distribution as
\begin{equation}
  %\boldsymbol{\theta} \sim \normaldist{\boldsymbol{\mu}}{\frac{1}{\tau} \boldsymbol{I}},
  \boldsymbol{\theta} \sim \normaldist{\boldsymbol{\mu}}{\sigma^2 \boldsymbol{I}},
\end{equation}
where $\boldsymbol{\mu} \in \mathbb{R}^3$, $\sigma^2 \in \mathbb{R}^+$ and $\boldsymbol{I}$ is a $3 \times 3$ identity matrix.
% In this context, $\tau$ is the inverse of the variance, and is called the \textbf{precision}.
Assuming $\boldsymbol{\mu} = \left[ \mu_0, \mu_1, \mu_2 \right]^T$, then the prior distribution takes the following form
\begin{equation}
  % \prob{\theta} = \prod_{k = 0}^2 \sqrt{\frac{\tau}{2 \pi}} e^{- \tau \frac{\left( \theta_k - \mu_k \right)^2}{2}}.
  \prob{\boldsymbol{\theta}} = \prod_{k = 0}^2 \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(\theta_k - \mu_k)^2}{2\sigma^2}}.
\end{equation}

The likelihood can be written as
\begin{equation}
  \prob{\boldsymbol{X} | \boldsymbol{\theta}} = \prod_{i = 1}^n  p_i(\boldsymbol{\theta})^{y_i}\left(1 - p_i(\boldsymbol{\theta}) \right)^{1 - y_i},
\end{equation}
which follows from the independence assumption in the data-generating mechanism. Moreover, Bayes' theorem in equation \eqref{eq:bayes_theorem_prop} allows one to write the posterior distribution as
%so, following Bayes' theorem in equation \eqref{eq:bayes_theorem_prop}, the posterior distribution is
\begin{equation}
  \prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}} =
    \frac
    {
      \left[ \prod_{i = 1}^n  p_i(\boldsymbol{\theta})^{y_i}\left(1 - p_i(\boldsymbol{\theta}) \right)^{1 - y_i} \right]
      \left[ \prod_{k = 0}^2 \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(\theta_k - \mu_k)^2}{2\sigma^2}} \right]
      % \left[ \prod_{k = 0}^2 \sqrt{\frac{\tau}{2 \pi}} e^{- \tau \frac{\left( \theta_k - \mu_k \right)^2}{2}} \right]
    }{
      \int \left[ \prod_{i = 1}^n  p_i(\boldsymbol{\theta})^{y_i}\left(1 - p_i(\boldsymbol{\theta}) \right)^{1 - y_i} \right]
      \left[ \prod_{k = 0}^2 \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(\theta_k - \mu_k)^2}{2\sigma^2}} \right] d\boldsymbol{\theta}
      % \left[ \prod_{k = 0}^2 \sqrt{\frac{\tau}{2 \pi}} e^{- \tau \frac{\left( \theta_k - \mu_k \right)^2}{2}} d\theta
    }.
\end{equation}

It should be noted that the integral in the denominator is a constant with respect to $\boldsymbol{\theta}$, thus, the posterior distribution can be computed up to a normalizing constant as
%Most of them rely on the fact that the denominator is only a normalizing constant and only use the equation as a proportion
\begin{equation}
  \prob{\boldsymbol{\theta} | \boldsymbol{y}, \boldsymbol{X}} \propto
  \left[ \prod_{i = 1}^n  p_i(\boldsymbol{\theta})^{y_i}\left(1 - p_i(\boldsymbol{\theta}) \right)^{1 - y_i} \right]
  \left[ \prod_{k = 0}^2 \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(\theta_k - \mu_k)^2}{2\sigma^2}} \right].
  %\left[ \prod_{k = 0}^2 \sqrt{\frac{\tau}{2 \pi}} e^{- \tau \frac{\left( \theta_k - \mu_k \right)^2}{2}} \right].
\end{equation}

Unfortunately, for most practical problems the integral in the denominator cannot be solved analytically, therefore numerical methods are used. One class of widely used algorithms is Markov Chain Monte Carlo (MCMC), which includes Metropolis-Hastings and Hamiltonian Monte Carlo. The main goal of MCMC methods is to generate samples in an attempt to summarize and report posterior inferences on the unknown parameters. Another class of numerical methods is Variational Inference, which is discussed in Chapter \ref{ch:variational_inference}.

In general, when training a model for prediction, it is not the main goal to minimize training error, but prediction error. It does not matter whether one chooses to follow the function minimization, frequentist or Bayesian paradigm. That is, one wants to minimize the error of any future observation $(\boldsymbol{x}^*, \boldsymbol{y}^*)$. This is because these models are trained to be used with data that has not been seen before, therefore, it is desirable to have a model that generalizes well to future observations. This means that one wants to minimize the expected prediction error, hence, it is common to divide the data set $(\boldsymbol{X}, \boldsymbol{y})$ in two: the training set and the test set. To do this, from the original data set, a random sample of observations is assigned to be part of the training set and the rest are part of the test set. The idea is that the test set should be used to measure the predictive performance of the model, and this also helps to diagnose an overfitted model, a concept which is discussed later in this chapter. The data is sometimes divided in three different sets. This third set is called the validation set, and it is used to measure the generalization performance when tuning hyperparameters, such as regularization parameter values, discussed further in this document.

The idea of separating the data in different sets comes from the \textbf{bias-variance trade-off}, a property of learning models. The idea of the bias-variance trade-off is that the expected prediction mean squared error of a learned model $\hat{f}$ can be decomposed in the sum of three quantities: the squared bias of $\hat{f}$, the variance of $\hat{f}$, and an irreducible error. The bias of the model refers to the error that comes from approximating a complicated phenomenon with a simpler model, the variance refers to how much the estimated model $\hat{f}$ would change with a different data set, and the irreducible error refers to the noise that comes from the problem itself which cannot be decreased \cite{friedman2001elements} \cite{james2013introduction}. In general, a modeler wants to have models with low bias and low variance, so that the expected prediction mean squared error is low, but as the name implies, there is a trade-off. Flexible models tend to have low bias but high variance, and rigid models tend to have low variance but high bias.

When training highly flexible models, the separation of data in training and validation sets is more important. This is because very flexible models tend to have higher variance, and thus they may fit the training data very well, but may not generalize very well with data that has not been previously observed. This is due to the fact that part of the error is being captured by the model. This concept is known as \textbf{overfitting}, and is a very common problem in machine learning applications.
%The problem with an overfitted model is that it will give bad predictions with new data, thus overfitted models have a small training error, but a bigger test error.



\subsection{Regularization}

One way to control overfitting is with \textbf{regularization}, in which a penalization term is added to the loss function to prevent the parameters from taking values that are too high. A very simple regularizer is the $L_2$ norm in which the sum of the squared parameters is added to the loss function.

For example, in linear regression, the usual loss function that is sought to be minimized is
\begin{equation}
  \label{eq:lin_reg_loss_funct}
  \sum_{i = 1}^n{ \left( y_i - \sum_{k = 1}^p  \theta_k x_k \right) ^ 2},
\end{equation}
but with the $L_2$ regularizer (also known as ridge regression), equation \eqref{eq:lin_reg_loss_funct} is modified so that the loss function now is
\begin{equation}
  \label{eq:lin_reg_loss_funct_reg}
  \sum_{i = 1}^n{ \left( y_i - \sum_{k = 1}^p \theta_k x_k \right) ^ 2}
  + \lambda \left( \sum_{k = 2}^p \theta_k^2 \right),
\end{equation}
where $\lambda > 0$ is called the regularization parameter, and controls the relative importance between the two main terms of the sum. The value of this parameter can have a big impact in the loss function: if it is too big, then it will penalize too much and most parameters will be near 0, but if it is too small, then there is little regularization effect and it is as if there were no penalization at all. In practice, it is common to choose several concrete values of $\lambda$ (such as 0.001, 0.01 and 0.1) and select the final value using the validation set previously mentioned, so that the final chosen value of $\lambda$ minimizes the validation set error.

Another type of regularization that is widely used in neural network literature, presented further in this work, is \textbf{dropout} \cite{srivastava2014dropout}. Dropout consists in randomly dropping out, that is temporarily removing, units of the neural network in each feed-forward mini-batch pass, which results in a thinned network. Each unit is removed, along with all its input and output connections, with a certain probability $p$. The original idea of dropout is that by randomly dropping units, the modeler is essentially training different networks, so the end result is the combination of different architectures, which result in less overfitting. At test time, the prediction is usually just the average of the different thinned networks. These concepts will be clearer after Chapter \ref{ch:ann} in which neural networks are discussed.
