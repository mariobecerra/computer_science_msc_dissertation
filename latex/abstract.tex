\begin{abstract}{english}

  In this work, Bayesian methods in Deep Learning are compared with traditional methods in the context of Active Learning. Traditional methods are based on the optimization of a loss function and only offer a single point estimate with no measure of uncertainty in the prediction or the parameters, whereas the Bayesian approach gives a full posterior distribution for each of the parameters, thus giving as well a posterior predictive distribution of the response variable.

  Recent work showed that using dropout in neural networks is equivalent to a variational approximation to a Bayesian neural network, hence offering a scalable approach to sampling from the posterior distribution. This approach can be used with Convolutional Neural Networks, an architecture widely used for image classification tasks.

  The Bayesian and frequentist CNNs are compared in three image datasets, using the model's information to query the most useful images from a large pool of unlabeled data to add to a set of training data and achieve higher accuracy than a random selection of images. Our results show that using the model is better than randomly choosing images, but they also show that no difference can be seen between the frequentist and Bayesian paradigms in this setting.

\end{abstract}



\begin{abstract}{spanish}
	Traducir resumen.
\end{abstract}
